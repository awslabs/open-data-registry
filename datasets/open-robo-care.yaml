Name: OpenRoboCare Multi-Modal Expert Demonstration Dataset for Robot-Assisted Caregiving
Description: A comprehensive multi-modal dataset capturing real-world caregiving routines from 21 occupational therapists performing 15 daily caregiving tasks. The dataset includes synchronized RGB-D video, tactile sensing, eye-gaze tracking, pose annotations, and action labels across 315 sessions totaling 19.8 hours of expert demonstrations. Data modalities include anonymized RGB images, depth maps, 44-sensor tactile readings, 2D/3D pose tracking, temporal action annotations, and first/third-person videos, enabling research in robot learning from demonstration, multimodal perception, and safe human-robot interaction for caregiving applications.
Documentation: https://emprise.cs.cornell.edu/robo-care/docs
Contact: https://emprise.cs.cornell.edu/robo-care/
ManagedBy: "[EmPRISE Lab at Cornell University](https://emprise.cs.cornell.edu/)"
UpdateFrequency: Static dataset - no regular updates planned
Tags:
  - computer vision
  - robotics
  - machine learning
  - health
License: "BSD-3-Clause license - Academic and non-commercial use permitted. See documentation for full terms."
Citation: "Liang, X., Liu, Z., Lin, K., Gu, E., Ye, R., Nguyen, T., Hsu, C., Wu, Z., Yang, X., Cheung, C.S.Y., Soh, H., Dimitropoulou, K., & Bhattacharjee, T. (2025). OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)."
Resources:
  - Description: Full Dataset
    ARN: arn:aws:s3:::openrobocare
    Region: us-east-1
    Type: S3 Bucket
DataAtWork:
  Tutorials:
    - Title: OpenRoboCare Dataset Viewer
      URL: https://emprise.cs.cornell.edu/robo-care-viewer/
      AuthorName: Cornell University EmPRISE Lab
      AuthorURL: https://emprise.cs.cornell.edu/
  Publications:
    - Title: "OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving"
      URL: https://emprise.cs.cornell.edu/robo-care/
      AuthorName: Liang X, Liu Z, Lin K, et al.