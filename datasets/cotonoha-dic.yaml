Contact: polm@cotonoha.io
DOI: 10.60489/roda.ex0708
DataAtWork:
  Publications:
  - AuthorName: Paul O'Leary McCann
    AuthorURL: https://dampfkraft.com
    Title: How to Tokenize Japanese in Python
    URL: https://www.dampfkraft.com/nlp/how-to-tokenize-japanese.html
  Tools & Applications:
  - AuthorName: Paul O'Leary McCann
    AuthorURL: https://cotonoha.io
    Title: unidic-py
    URL: https://github.com/polm/unidic-py
  Tutorials:
  - AuthorName: Paul O'Leary McCann
    AuthorURL: https://cotonoha.io
    Services:
    - SageMaker
    Title: Fugashi Word Count Tutorial
    URL: https://github.com/polm/fugashi-sagemaker-demo/blob/master/fugashi%20wordcount.ipynb
Description: Japanese Tokenizer Dictionaries for use with MeCab.
Documentation: 'This dataset includes dictionaries for tokenization and morphological

  analysis of Japanese for use with MeCab. This includes NINJAL''s UniDic, a

  modified smaller version of UniDic for situations that require it, and the

  legacy IPADic dictionary.

  '
License: "Versions of Unidic offered here are available under the GPL/LGPL/BSD license.\n\
  \nIPADic is offered under a unique BSD-like license. See below. \n\n    https://github.com/polm/ipadic-py/blob/master/ipadic/dicdir/COPYING\n"
ManagedBy: Cotonoha
Name: Japanese Tokenizer Dictionaries
Resources:
- ARN: arn:aws:s3:::cotonoha-dic
  Description: Dictionary Files
  Region: ap-northeast-1
  Type: S3 Bucket
Tags:
- aws-pds
- natural language processing
- csv
- japanese
UpdateFrequency: Infrequently (typically less than once a year)
